function [CCF,forestPredictsTest,treePredictsTest,cumulativeForestPredictsTest] = genCCF(XTrain,YTrain,nTrees,optionsFor,iFeatureNum,XTest)
%genCCF Generate a canonical correlation forest
%
% [CCF, forPred, treePred, cumForPred] = genCCF(XTrain,YTrain,nTrees,...
%                                             options,iFeatureNum,XTest)
%
% Creates a canonical correlation forest (CCF) comprising of nTrees
% canonical correlation trees (CCT) containing splits based on the a CCA
% analysis between the training data and a binary representation of the
% class labels.
%
% Required Inputs: 
%         XTrain = Array giving training features.  Each row should be a
%                  seperate data point and each column a seperate feature.
%                  Categorical features must be processed before calling
%                  genCCF using the processInputData function.
%         YTrain = Binary representation of classes.  Each row is a
%                  seperate data point and contains only a single non zero
%                  term, the column of which indicates the class.
%         nTrees = Number of trees to create
%
% Options Inputs:
%        options = Options object created by optionsClassCCT.  If left
%                  blank then a default set of options corresponding to the
%                  method detailed in the paper is used.
%    iFeatureNum = Vector for grouping of categorical variables as
%                  generated by processInputData function.  If left blank 
%                  then each column of XTrain is treated as a standalone
%                  feature.
%          XTest = Test data to make predictions for.  If the input
%                  features for the test data are known at test time then
%                  using this input with the option bKeepTrees = false can
%                  significantly reduce the memory requirement.
%
% Outputs:
%            CCF = Cell array of CCTs.  Forest prediction can be made using
%                  predictFromCCF function or individual trees using the
%                  predictFromCCT function.
%        forPred = Complete forest predictions for XTest
%       treePred = Individual tree predictiosn for XTest
%     cumForPred = Predictions of forest for XTest cumulative in the
%                  individual trees.  cumForPred(:,end)==forPred
%
% 10/06/15


if ~isnumeric(XTrain) || any(isnan(XTrain(:))) || any(isnan(XTest(:)))
    error('Data is not processed, please use processInputData function');
end

if ~exist('iFeatureNum','var') || isempty(iFeatureNum)
    % If no grouping of columns to individual features is given presume
    % each is an independent feature.
    iFeatureNum = 1:size(XTrain,2);
end

N = size(XTrain,1);
D = numel(unique(iFeatureNum)); % Note that setting of number of features to subsample is based only 
                                % number of features before expansion of categoricals.

if size(YTrain,2)==1
    baseCounts = [sum(~YTrain),sum(YTrain)];
else
    baseCounts = sum(YTrain,1);
end
    
if ~exist('optionsFor','var') || isempty(optionsFor)
    optionsFor = optionsClassCCT(D,baseCounts);
else
    optionsFor = optionsFor.updateForD(D);
    optionsFor = optionsFor.updateForpWhenEven(baseCounts);
end

nOut = nargout;

if nOut<2 && ~optionsFor.bKeepTrees
    optionsFor.bKeepTrees = true;
    warning('Selected not to keep trees but only requested a single output of the trees, reseting optionsFor.bKeepTrees to true');
end

if nOut>1 && (~exist('XTest','var') || isempty(XTest))
    error('To return more than just the trees themselves, must input the test points');
end

CCF = cell(1,nTrees);
treePredictsTest = NaN(size(XTest,1),nTrees);

if optionsFor.bUseParallel == true    
    parfor nT = 1:nTrees
        warning('off','MATLAB:nearlySingularMatrix');
        warning('off','MATLAB:singularMatrix');
        
        [tree,predicts] = genTree(XTrain,YTrain,optionsFor,iFeatureNum,N,nOut);        
        if optionsFor.bKeepTrees
            CCF{nT} = tree;
        end        
        if nOut>1
            treePredictsTest(:,nT) = predicts;
        end        
    end
else
    warning('off','MATLAB:nearlySingularMatrix');
    warning('off','MATLAB:singularMatrix');
    
    for nT = 1:nTrees
        [tree,predicts] = genTree(XTrain,YTrain,optionsFor,iFeatureNum,N,nOut);        
        if optionsFor.bKeepTrees
            CCF{nT} = tree;
        end        
        if nOut>1
            treePredictsTest(:,nT) = predicts;
        end
    end
end

if nOut<2
    return
end

% If requested, calculate the forest predictions for XTest.  This is based
% on an equally weighted voting system.
K = min(2,size(YTrain,2));
treePredThis = bsxfun(@eq,treePredictsTest,reshape(1:K,[1,1,K]));
YcumProbs = bsxfun(@rdivide,cumsum(treePredThis,2),reshape(1:nTrees,[1,nTrees,1]));
[~,iMax] = max(YcumProbs,[],3);
cumulativeForestPredictsTest = squeeze(iMax);
forestPredictsTest = cumulativeForestPredictsTest(:,end);

end

function [tree,predicts] = genTree(XTrain,YTrain,optionsFor,iFeatureNum,N,nOut)

if options.bBagTrees
    iTrainThis = datasample(1:N,N);
else
    iTrainThis = 1:N;
end

XTrainBag = XTrain(iTrainThis,:);
YTrainBag = YTrain(iTrainThis,:);

if optionsFor.bApplyRotForPreprocess
    % This allows functionality to use the Rotation Forest algorithm as a
    % meta method for individual CCTs
    [R,muX,XTrainRot] = rotationForestDataProcess(XTrainBag,optionsFor.RotForM,optionsFor.RotForpS,optionsFor.RotForpClassLeaveOut);
    XTrainBag = XTrainRot;
    rotForDetails = struct('R',R,'muX',muX);
end

tree = growTree(XTrainBag,YTrainBag,optionsFor,iFeatureNum,0);

if optionsFor.bApplyRotForPreprocess
    tree.rotForDetails = rotForDetails;
end

if nOut>1
    predicts = predictFromCCT(tree,XTest);
else
    predicts = [];
end

end
